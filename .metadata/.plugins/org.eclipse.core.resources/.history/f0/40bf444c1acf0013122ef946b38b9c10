package sample;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class KMeansReducer 
extends Reducer<IntWritable,Text,IntWritable,Text> {

	int newCentroid0 = 0;
	int newCentroid1 = 0;
	int newCentroid2 = 0;
	public void reduce(IntWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
     
	//input will be the centroid as key and the user specification as value
	//key = 50; value = uowusers 17 low	
	
	//Average the number of followers for each centroid and emit new centroid
		int totalFollowers = 0;
		int count = 0;
		String[] line;
		for(Text value: values){
			line = values.toString().split(" ");
			totalFollowers += Integer.parseInt(line[1]);
			++count;
		}
		if(newCentroid0 == 0){
			newCentroid0 = totalFollowers/count;
		}else if(newCentroid1 == 0){
			newCentroid1 = totalFollowers/count;
		}else{
			newCentroid2 = totalFollowers/count;
		}
		
//	System.out.println(key );	
//     int counter=0;
//     for(Text value :values)
//     {
//    	 
//    	 counter++;
//     }
	}
}
